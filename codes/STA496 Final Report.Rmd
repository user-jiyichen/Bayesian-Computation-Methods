---
title: "STA496 Project Report: Parameter Estimation for Logistic Regression via Various Bayesian Computation Methods"
author: 'Yichen Ji'
output: html_document
---

```{r,echo=FALSE,warning=FALSE,include=FALSE}
# rm(list = ls())
library(Rlab)
library(mvtnorm)
library(ggplot2)
library(coda)
library(bayesplot)
library(mvtnorm)  # for multivariate normal density
library(pracma)   # for 2d-integral, sqrtm
library(tidyverse)
library(BayesLogit)
library(PolyaGamma)
```


In this report, I will summarize and compare several Bayesian computation techniques in the context of Bayesian logistic regression. The techniques include Markov Chain Monte Carlo(MCMC), Approximate Bayesian Computation methods(ABC), Bayesian Synthetic Likelihood(BSL) and Variational Inference(VI). 

Consider a Bayesian logistic regression model with 5 covariates with the corresponding parameters. The parameters are denoted as $\beta_k, k\in\{0,1,2,3,4\}$, and are drawn from a standard multivariate normal prior distribution i.e. zero mean and identity covariance matrix. The covariates $X_l, l\in\{1,2,3,4\}$ are IID generated from a continuous uniform distribution $X_l \sim Unif[-1,1]$ and 10 Bernoulli outcomes are simulated from this logistic model. In other words, to produce $\{y_i\}_{i=1}^{10}$,

$$\boldsymbol{\beta}\sim\mathcal{N}(\boldsymbol{0,I})\ \& \ X_{li} \sim Unif[-1,1]\Rightarrow p=\frac{1}{1+e^{-\boldsymbol{X_i}^T\boldsymbol{\beta}}} \Rightarrow y_i\sim Ber(p)$$

The performance of algorithms is evaluated in terms of mean squared error(MSE). At each round, m=5000 MCMC parameter samples are simulated for each algorithm and the sample mean and sample variance of each parameter$\beta_k$ are collected. MSE of each round are calculated by definition $MSE(\beta_k)^{(i)} = Bias^{(i)}(\hat{\beta_k},\beta_{k,real})^2+Var^{(i)}(\hat{\beta_k})$. For each algorithm, we run 200 rounds and summarize their MSEs of all rounds in terms of mean, median and quantile values.

We first state the result:

1. The benchmark Polya-Gamma MCMC sampler returns the second smallest MSE among all five parameters, but it is quite slow(~20 minutes to simulate for 200 rounds, each with 5000 MCMC parameter samples).

2. The MCMC-ABC method is only accurate for the intercept parameter$\beta_0$ while all others are poorly drawn from the posterior with high mean squared errors.

3. The Bayesian synthetic likelihood method has the largest mean squared errors for all parameters. The MCMC-BSL method, by definition, uses a Gaussian approximation for the distribution of the summary statistics, which is not quite suitable for binary logistic data.

4. The coordinate ascent mean-field variational interfence(CAVI) for our logistic model performs the beat among all five parameters, beating the benchmark Polya-Gamma MCMC sampler. Moreover, it is best in time efficiency, running in about one minute. 

```{r,echo=FALSE,warning=FALSE,include=FALSE}
# Some distance functions
abs_diff = function(x,y){return(sum(abs(x-y)))}
square_diff = function(x,y){return(sum(x-y)^2)}
KL_div = function(x,y){return(sum(x * log(x/y)))}


# number of simulations/ iterations
num_sim = 5000

# dimension of data X for the logistic regression
dim_data = 10
# true parameter values i.e. betas
observed_beta = c(1,1,1,1,1)
```


## Bayesian Logistic Regression Simulator

The true/observed parameters values are $\beta_{k,real}=1 \quad \forall k\in\{0,1,2,3,4\}$and true binary data set generated by these parameters are $ \{0,1,1,0,1,0,1,1,0,0\} $. The codes for implementation are in the Code Appendix.
```{r,echo=FALSE,warning=FALSE,include=FALSE}
######################### Logistic Generative Model#########################
set.seed(123)

sim_logistic_data = function(n = dim_data, beta) {
# generate xi's
x1 = runif(n, -1, 1);x2 = runif(n, -1, 1);x3 = runif(n, -1, 1);x4 = runif(n, -1, 1)
# the design matrix including the intercept
x = cbind(1,x1,x2,x3,x4) 
# linear part
eta =  data.matrix(x) %*% data.matrix(beta)
# probability as Sigmoid
p = 1 / (1 + exp(-eta)) 
# binary samples
y = rbern(n, prob = p)
return(y)}

observed_data = sim_logistic_data(dim_data,observed_beta)
```

```{r,echo=FALSE,warning=FALSE,include=FALSE}
observed_data
```



## Polya-Gamma MCMC sampler

MCMC methods draw samples from a proposal density, building then an ergodic Markov chain whose stationary distribution is the desired distribution by accepting or rejecting those candidate samples as the new state of the chain.

Choi and Hobert (2013) have proven that the Polya-Gamma Gibbs sampler for Bayesian logistic regression is uniformly ergodic.It guarantees the existence of a central limit theorem for Monte Carlo averages of posterior draws.

Polson, Scott and Windle(2013) have shown that: In simple logit models with abundant data and no hierarchical structure, the Polya-Gamma method is a close second to the independence Metropolis-Hastings (MH) sampler, as long as the MH proposal distribution is chosen carefully.

To fix notation: let $y_{i}$ be the number of successes, $n_{i}$ the number of trials, and $x_{i}=$ $\left(x_{i 1}, \ldots, x_{i p}\right)$ the vector of covariates for observation $i \in\{1, \ldots, N\} .$ Let 
$y_{i} \sim \operatorname{Binom}\left(n_{i}, 1 /\{1+\right.\left.\left.e^{-\psi_{i}}\right\}\right)$, where $\psi_{i}=x_{i}^{T} \boldsymbol{\beta}$ are the log odds of success. Finally, let $\boldsymbol{\beta}$ have a Gaussian prior, $\boldsymbol{\beta} \sim \mathrm{N}(b, B)$. To sample from the posterior distribution using the Pólya-Gamma method, simply iterate two steps:
$$
\begin{aligned}
\left(\omega_{i} \mid \boldsymbol{\beta}\right) & \sim \mathrm{PG}\left(n_{i}, x_{i}^{T} \boldsymbol{\beta}\right) \\
(\boldsymbol{\beta} \mid y, \omega) & \sim \mathrm{N}\left(m_{\omega}, V_{\omega}\right),
\end{aligned}
$$
where
$$
\begin{aligned}
V_{\omega} &=\left(X^{T} \Omega X+B^{-1}\right)^{-1} \\
m_{\omega} &=V_{\omega}\left(X^{T} \kappa+B^{-1} b\right)
\end{aligned}
$$


```{r,echo=FALSE,warning=FALSE,include=FALSE}
######################### Polya-Gamma MCMC #########################
generate_from_logistic = function(n = dim_data, beta) {
x1 = runif(n, -1, 1);x2 = runif(n, -1, 1);x3 = runif(n, -1, 1);x4 = runif(n, -1, 1)

# the design matrix including the intercept
x = cbind(1, x1,x2,x3,x4) 

eta =  data.matrix(x) %*% data.matrix(beta)
# probability as Sigmoid
p = 1 / (1 + exp(-eta)) 
# binary trial samples
y = rbern(n, prob = p)
return(list(X=x, y=y))}


# Polya-Gamma sampler
data = generate_from_logistic(1000,observed_beta)
obj = gibbs_sampler(data$y, data$X, lambda = 0.001, n_iter_total = 250, burn_in = 50)
```

```{r,echo=FALSE,warning=FALSE,include=FALSE}
df = as.data.frame(obj$beta)
colnames(df) = c('beta0','beta1','beta2','beta3','beta4')
```


```{r,echo=FALSE,warning=FALSE,include=FALSE}
# calculate MSE for 200 rounds
set.seed(1)
data = generate_from_logistic(1000,observed_beta)

round = 200
sample_mean_PG = data.frame(beta0=double(),beta1=double(),beta2=double(),beta3=double(),beta4=double())
sample_var_PG = data.frame(beta0=double(),beta1=double(),beta2=double(),beta3=double(),beta4=double())

i=1
while(i <= round){
  obj = gibbs_sampler(data$y, data$X, lambda = 0.001, n_iter_total = 250, burn_in = 50)
  df = as.data.frame(obj$beta)
  colnames(df) = c('beta0','beta1','beta2','beta3','beta4')
  
  beta0 = df$beta0
  mean_beta0 = mean(beta0)
  var_beta0 = var(beta0)
  
  beta1 = df$beta1
  mean_beta1 = mean(beta1)
  var_beta1 = var(beta1)
  
  beta2 = df$beta2
  mean_beta2 = mean(beta2)
  var_beta2 = var(beta2)
  
  beta3 = df$beta3
  mean_beta3 = mean(beta3)
  var_beta3 = var(beta3)
  
  beta4 = df$beta4
  mean_beta4 = mean(beta4)
  var_beta4 = var(beta4)
  
  sample_mean_PG = add_row(sample_mean_PG, beta0 = mean_beta0,
                          beta1 = mean_beta1,
                          beta2 = mean_beta2,
                          beta3 = mean_beta3,
                          beta4 = mean_beta4)
  sample_var_PG = add_row(sample_var_PG, beta0 = var_beta0,
                          beta1 = var_beta1,
                          beta2 = var_beta2,
                          beta3 = var_beta3,
                          beta4 = var_beta4)
  i = i+1
}
# mean-squared error of each round for each beta
MSE_PG = (sample_mean_PG - observed_beta)^2 + sample_var_PG
round_mean_MSE_PG = sapply(MSE_PG, mean)
```

```{r,echo=FALSE,cache=TRUE}
summary(MSE_PG)
```
This summary table shows the performance of this methods in terms of mean squared errors(MSE) for 200 iterations, each generating 5000 MCMC samples. As a result, the Polya-Gamma MCMC algorithm gives a decent output in parameter estimation with all mean squared errors smaller than 0.1 for all 5 parameters. We can set this upshot as a benchmark to compare with other algorithms.


## Approximate Bayesian Computation(ABC)

The generic principle of ABC algorithms can be summarized as follows:

1. Create a simulation model of the stochastic process;

2. Specify a distribution for the prior of the parameter(s) to be estimated;

3. Run a simulation. Whenever the simulated outcome matches the observation (within a threshold tolerance level), accept the parameter value generated from the prior distribution, otherwise reject it;

4. The distribution of accepted values is the ABC-approximated posterior distribution.

I first implemented the ABC rejection algorithm, but it took hours to output the required number of MCMC samples. I think the low acceptance rate comes from the difficulty to exactly match the observation and the simulation dataset. In addition, if the data are too high dimensional we never observe simulations that are ‘close’ to the field data due to the curse of dimensionality.

Then following Marjoram(2003), a MCMC-ABC sampler is employed with a multivariate Gaussian transition kernel $q(\theta \rightarrow \theta')$. It is based on the following steps:

1. If now at $\theta$ propose a move to $\theta'$ according to the Gaussian transition kernel $q(\theta \rightarrow\theta')$;

2. Generate simulation dataset $\mathcal{D'}$ using the logistic generative model $\mathcal{M}$ with parameter $\theta'$;

3. If $\rho(\mathcal{D'},\mathcal{D}) \leq \epsilon$, calculate the acceptance probability $h=h\left(\theta, \theta^{\prime}\right)=\min \left(1, \frac{\pi\left(\theta^{\prime}\right) q\left(\theta^{\prime} \rightarrow \theta\right)}{\pi(\theta) q\left(\theta \rightarrow \theta^{\prime}\right)}\right)$; otherwise stay at $\theta$ and return the step 1;

4. Accept $\theta'$ with probability $h$ and otherwise, stay at $\theta$, then return to step 1.

Since the logistics output is a 0/1 binary dataset, summation is a reasonable pick of summary statistics for categorical variables, counting the number of successes(1's) regradless of the order of data within the dataset. We set the threshold tolerance level to be $\epsilon=1$ and the distance metric to be the absolute difference$\rho(X,Y)=|X-Y|$, meaning that we tolerate the difference up to $±1$ in the total number of successes regardless of their order in the dataset.

```{r,echo=FALSE,warning=FALSE,include=FALSE}
############################### ABC MCMC ##################################

# setup
M = 5000
y0 = observed_data
s0 = sum(y0)
epsilon = 1

MCMC.ABC = function(M, epsilon, real_data){
  
  # set initial state of MCMC
  beta0 = c(0,0,0,0,0)
  y_init = sim_logistic_data(dim_data,beta0)
  
  # sum/ count as summary statistics
  while (abs_diff(sum(y_init),sum(real_data) >= 2)){
    y_init = sim_logistic_data(dim_data,beta0)
  }
  chain = array(dim = c(M + 1, 5)) 
  chain[1,] = beta0
    
  # main loop
  for (t in 1:M+1){
    proposed_beta = c(rmvnorm(1, mean = chain[t-1,]))
    simulated_data = sim_logistic_data(dim_data, proposed_beta)
    distance = abs_diff(sum(simulated_data), s0)
    
    # use multinormal as the transition kernel
    log_num = log(dmvnorm(chain[t-1,], 
                            mean = proposed_beta)) + 
      log(dmvnorm(proposed_beta, mean = c(0,0,0,0,0)))
    
    log_denom = log(dmvnorm(proposed_beta, 
                           mean = chain[t-1,])) + 
      log(dmvnorm(chain[t-1,], mean = c(0,0,0,0,0)))
    
    mlgb = exp(log_num - log_denom)
    alpha = min(1, mlgb)
    u = runif(1)
    
    # reject/accept step
    if (u <= alpha & distance < epsilon){
      chain[t,] = proposed_beta
    } else{chain[t,]=chain[t-1,]}
  }
  df = as.data.frame(chain)
  colnames(df) = c('beta0','beta1','beta2','beta3','beta4')
  return(df)
}
```


```{r,echo=FALSE}
chain = MCMC.ABC(M,epsilon,y0)
mcmc_df = as.data.frame(chain)
colnames(mcmc_df) =  c('beta0','beta1','beta2','beta3','beta4')

mcmc_intervals(mcmc_df, pars = colnames(mcmc_df))
```

Running the ABC-MCMC algorithm for one round, the graph gives a taste of the distribution of the ABC-approximated posterior distribution of logistic parameters. It shows that only the accepted MCMC samples of $\beta_0$ perform as expected around the true parameter $\beta_{0,true}=1$and draws of other parameters center around zero.

Next, let's see how it goes for 200 rounds in terms of mean squared errors:

```{r,echo=FALSE,warning=FALSE,include=FALSE}
set.seed(1)
round = 200
sample_mean_ABC = data.frame(beta0=double(),beta1=double(),beta2=double(),beta3=double(),beta4=double())
sample_var_ABC = data.frame(beta0=double(),beta1=double(),beta2=double(),beta3=double(),beta4=double())

i=1
while(i <= round){
  df = MCMC.ABC(M,epsilon, y0)
  
  beta0 = df$beta0
  mean_beta0 = mean(beta0)
  var_beta0 = var(beta0)
  
  beta1 = df$beta1
  mean_beta1 = mean(beta1)
  var_beta1 = var(beta1)
  
  beta2 = df$beta2
  mean_beta2 = mean(beta2)
  var_beta2 = var(beta2)
  
  beta3 = df$beta3
  mean_beta3 = mean(beta3)
  var_beta3 = var(beta3)
  
  beta4 = df$beta4
  mean_beta4 = mean(beta4)
  var_beta4 = var(beta4)
  
  sample_mean_ABC = add_row(sample_mean_ABC, beta0 = mean_beta0,
                          beta1 = mean_beta1,
                          beta2 = mean_beta2,
                          beta3 = mean_beta3,
                          beta4 = mean_beta4)
  sample_var_ABC = add_row(sample_var_ABC, beta0 = var_beta0,
                          beta1 = var_beta1,
                          beta2 = var_beta2,
                          beta3 = var_beta3,
                          beta4 = var_beta4)
  i = i+1
}
# mean-squared error of each round for each beta
MSE_ABC = (sample_mean_ABC - observed_beta)^2 + sample_var_ABC
round_mean_MSE_ABC = sapply(MSE_ABC, mean)
```

```{r,echo=FALSE,cache=TRUE}
summary(MSE_ABC)
```
The summary table indicates that except for $\beta_0$, the mean squared errors are relatively large comparing to the benchmark Polya-Gamma MCMC algorithm. I tried to shrink the tolerance threshold $\epsilon$, use different summary statistics such as mean or the identical dataset, and change different distance functions such as squared distance or K-L divergence. However, the performances of different combination of trials are either even worse, nearly the same or take forever to finish the run execution.


## Bayesian Synthetic Likelihood(BSL)

Bayesian Synthetic Likelihood method by definition uses a multivariate Normal approximation, which depends only on a single tuning parameter, n, the number of replicated simulations of the model used in estimating the mean and covariance matrix of the multivariate Gaussian auxiliary model, denoted as $\boldsymbol{\mu}_{n}(\boldsymbol{\theta})=\frac{1}{n} \sum_{i=1}^{n} \boldsymbol{s}_{i}, \quad \boldsymbol{\Sigma}_{n}(\boldsymbol{\theta})=\frac{1}{n-1} \sum_{i=1}^{n}\left(\boldsymbol{s}_{i}-\boldsymbol{\mu}_{n}(\boldsymbol{\theta})\right)\left(\boldsymbol{s}_{i}-\boldsymbol{\mu}_{n}(\boldsymbol{\theta})\right)^{\top}$. BSL uses the following estimate $$p_{A, n}\left(\boldsymbol{s}_{y} \mid \boldsymbol{\theta}\right)=\mathcal{N}\left(\boldsymbol{s}_{y} ; \boldsymbol{\mu}_{n}(\boldsymbol{\theta}), \boldsymbol{\Sigma}_{n}(\boldsymbol{\theta})\right)$$
in place of the likelihood function of the summary statistics/data. According to Price et al.(2018), BSL samples from the target distribution $$
p_{A, n}\left(\boldsymbol{\theta} \mid \boldsymbol{s}_{\boldsymbol{y}}\right) \propto p_{A, n}\left(\boldsymbol{s}_{\boldsymbol{y}} \mid \boldsymbol{\theta}\right) p(\boldsymbol{\theta}),
$$
where
$$
p_{A, n}\left(\boldsymbol{s}_{y} \mid \boldsymbol{\theta}\right)=\int_{\mathrm{S}^{n}} \mathcal{N}\left(\boldsymbol{s}_{y} ; \boldsymbol{\mu}_{n}(\boldsymbol{\theta}), \boldsymbol{\Sigma}_{n}(\boldsymbol{\theta})\right) \prod_{i=1}^{n} p\left(\boldsymbol{s}_{i} \mid \boldsymbol{\theta}\right) d \boldsymbol{s}_{1: n}$$

Here I implemented the MCMC-BSL algorithm in Drovandi et al.(2018) in the following steps:
$$\begin{aligned}
&\text { 1: Simulate } s_{1: n} \stackrel{\text { iid }}{\sim} p\left(\cdot \mid \theta^{0}\right) \\
&\text { 2: Compute } \phi^{0}=\left(\mu_{n}\left(\theta^{0}\right), \Sigma_{n}\left(\theta^{0}\right)\right) \\
&\text { 3: for } i=1 \text { to } T \text { do } \\
&\text { 4: } \quad \text { Draw } \theta^{*} \sim q\left(\cdot \mid \theta^{i-1}\right) \\
&\text { 5: } \quad \text { Simulate } s_{1: n}^{*} \sim p\left(\cdot \mid \theta^{*}\right) \\
&\text { 6: } \quad \text { Compute } \phi^{*}=\left(\mu_{n}\left(\theta^{*}\right), \Sigma_{n}\left(\theta^{*}\right)\right) \\
&\text { 7: } \quad \text { Compute } r=\min \left(1, \frac{\mathcal{N}\left(s_{\text {obs }} ; \mu_{n}\left(\theta^{*}\right), \Sigma_{n}\left(\theta^{*}\right)\right) p\left(\theta^{*}\right) q\left(\theta^{i-1} \mid \theta^{*}\right)}{\mathcal{N}\left(s_{\text {obs }} ; \mu_{n}\left(\theta^{i-1}\right), \Sigma_{n}\left(\theta^{i-1}\right)\right) p\left(\theta^{i-1}\right) q\left(\theta^{*} \mid \theta^{i-1}\right)}\right) \\
&\text { 8: } \quad \text { if } \mathcal{U}(0,1)<r \text { then } \\
&\text { 9: } \quad \text { Set } \theta^{i}=\theta^{*} \text { and } \phi^{i}=\phi^{*} \\
&\text { 10: } \quad \text { else } \\
&\text { 11: } \quad \text { Set } \theta^{i}=\theta^{i-1} \text { and } \phi^{i}=\phi^{i-1} \\
&\text { 12: } \quad \text { end if } \\
&\text { 13: end for }
\end{aligned}$$


The inputs required are:

1. summary statistics $s_{obs}$: Since we need a continuous random variable to fit a Gaussian approximation, we cannot directly fit with the binary logistic datasets, so I choose to use the mean value as the summary statistic for Gaussian approximation. 

2. prior distribution $p(\beta)$: standard multivariate Normal distribution;

3. proposal distribution $q(\theta^*|\theta^{i-1})$: same as the proposal distribution in ABC-MCMC, that is, a Gaussian transition kernel;

4. number of iterations T = 5000;

5. initial value of the chain $\theta^0$: same as the initial state of ABC-MCMC by running a rejection-ABC method.

```{r,echo=FALSE,warning=FALSE,include=FALSE}
set.seed(1)
# a helper function to simulate n summary statistics
summary_stat_simulator = function(theta, n){
  summary_stat = c()
  for (i in 1:n){
    sim_data = sim_logistic_data(10, theta)
    summary_stat = c(summary_stat,mean(sim_data))
  }
  return(summary_stat)
}


MCMC.BSL = function(M, n, obs_summary_stat){
  
  # set initial state of MCMC
  beta_init = c(0,0,0,0,0)
  chain = array(dim = c(M, 5)) 
  chain[1,] = beta_init
  
  # simulate n summary statistics
  summary_sta = summary_stat_simulator(beta_init,n)
  
  # compute sample mean and sample variance
  sample_mean0 = mean(summary_sta)
  sample_var0 = var(summary_sta)
  
  # another chain for sample mean and variance
  phi_chain = array(dim = c(M, 2))
  phi_chain[1,1] = sample_mean0
  phi_chain[1,2] = sample_var0
  
  # main loop
  for (t in 2:M){
    proposed_beta = c(rmvnorm(1, mean = chain[t-1,]))
    summary_sta_t = summary_stat_simulator(proposed_beta, n)
    sample_mean_t = mean(summary_sta_t)
    sample_var_t = var(summary_sta_t)
    
    # use multinormal as the transition kernel
    log_num = 
      dnorm(obs_summary_stat,
            mean = sample_mean_t, 
            sd = sqrt(sample_var_t))+
      log(dmvnorm(chain[t-1,], mean = proposed_beta)) + 
      log(dmvnorm(proposed_beta, mean = c(0,0,0,0,0)))
    
    log_denom = dnorm(obs_summary_stat, 
                      mean = phi_chain[t-1,1], 
                      sd = phi_chain[t-1,2])+
      log(dmvnorm(proposed_beta,mean = chain[t-1,]))+
      log(dmvnorm(chain[t-1,], mean = c(0,0,0,0,0)))
    
    rej_rate = exp(log_num - log_denom)
    r = min(1, rej_rate)
    u = runif(1)
    
    # reject/accept step
    if (u < r){
      chain[t,] = proposed_beta
      phi_chain[t,1] = sample_mean_t
      phi_chain[t,2] = sample_var_t
      
    } else{chain[t,]=chain[t-1,]
    phi_chain[t,1] = phi_chain[t-1,1]
      phi_chain[t,2] = phi_chain[t-1,2]
    }
  }
  df = as.data.frame(chain)
  colnames(df) = c('beta0','beta1','beta2','beta3','beta4')
  return(df)
}
```


```{r,echo=FALSE,warning=FALSE,include=FALSE}
set.seed(1)

# setup
M = 5000 # number of iterations
n=30 # number of simulations for summary statistics
y0 = observed_data
s0 = mean(y0) # use mean as the (continuous) summary statistic
round = 200

sample_mean_BSL = data.frame(beta0=double(),beta1=double(),beta2=double(),beta3=double(),beta4=double())
sample_var_BSL = data.frame(beta0=double(),beta1=double(),beta2=double(),beta3=double(),beta4=double())

i=1
while(i <= round){
  df = MCMC.BSL(M,n,s0)
  
  beta0 = df$beta0
  mean_beta0 = mean(beta0)
  var_beta0 = var(beta0)
  
  beta1 = df$beta1
  mean_beta1 = mean(beta1)
  var_beta1 = var(beta1)
  
  beta2 = df$beta2
  mean_beta2 = mean(beta2)
  var_beta2 = var(beta2)
  
  beta3 = df$beta3
  mean_beta3 = mean(beta3)
  var_beta3 = var(beta3)
  
  beta4 = df$beta4
  mean_beta4 = mean(beta4)
  var_beta4 = var(beta4)
  
  sample_mean_BSL = add_row(sample_mean_BSL, beta0 = mean_beta0,
                          beta1 = mean_beta1,
                          beta2 = mean_beta2,
                          beta3 = mean_beta3,
                          beta4 = mean_beta4)
  sample_var_BSL = add_row(sample_var_BSL, beta0 = var_beta0,
                          beta1 = var_beta1,
                          beta2 = var_beta2,
                          beta3 = var_beta3,
                          beta4 = var_beta4)
  i = i+1
}
# mean-squared error of each round for each beta
MSE_BSL = (sample_mean_BSL - observed_beta)^2 + sample_var_BSL
round_mean_MSE_BSL = sapply(MSE_BSL, mean)
```

```{r, echo=FALSE,cache=TRUE}
summary(MSE_BSL)
```

The above table indicates that the MCMC-BSL method may not be a good choice for logistic likelihoods. By construction, it assumes a Normal approximation for the distribution of the summary statistics, so we need to pick a continuous statistic rather than discrete integer values like the original dataset. Here I choose the mean as the summary statistic, but the intuition tells us that the mean value is not sufficient to summarize binary categorical variables. We can investigate more on the choice of summary statistics for BSL methods in general.

## Coordinate Ascent Variational Inference(CAVI)

The K-L divergence is a measure of proximity/similarity between two distributions. Note that, this measure is not symmetric in general i.e.$KL(q||p) \neq KL(p||q)$, and minimized when two distributions are the same i.e. $KL(q||p) = 0$ when $q(\cdot) = p(\cdot)$.$$D_{\mathrm{KL}}(P \| Q)=\int_{-\infty}^{\infty} p(x) \log \left(\frac{p(x)}{q(x)}\right) dx =\sum_{x \in \mathcal{X}} p(x) \log \left(\frac{p(x)}{q(x)}\right)$$. The expectation is with respect to the left distribution in the expression.

Variational Inference turns an inference program into an optimization problem, by finding a distribution of latent variables $q^*(z)$ that minimizes the K-L Divergence between $q^*(z)$ and the true posterior conditional distribution $p(z|x)$. That is, $$q^*(z) = argmin_{q(z)\in Z} KL(q(z)||p(z|x))$$ However, this minimization problem is not computable because we have the evidence term $p(x)$ that is intractable. The vanilla form is called the coordinate ascent variational inference method, which can be implemented as follows:

* Input: A model $p(x,z)$, a data set $x$

* Initialize: Variational factors $q_j(z_j)$

* While the ELBO has not converged do
  + for $j \in\{1,...,m\}$ do
    -   Set $q_j(z_j) \propto exp\{\mathbb{E}_{-j}[logp(z_j|\mathbf{z_{-j}},\mathbf{x})]\}$
  + end
  + Compute $ELBO(q) =\mathbb{E}[\log p(\mathbf{z}, \mathbf{x})]+\mathbb{E}[\log q(\mathbf{z})]$
* end
* return $q(\mathbf{z})$

Due to the fact that the Gaussian prior and logistic likelihood are not conditionally conjugate pairs within an exponential family, the above CAVI algorithm for conjugate pairs in Blei's paper is not applicable for our problem setting. Instead, I found a nice paper 'Conditionally conjugate mean–field variational Bayes for logistic models' by Daniele Durante and Tommaso Rigon, and implemented the Algorithm 2: CAVI for logistic regression in this paper. The mean–field assumption allows the implementation of CAVI, which sequentially maximizes the evidence lower bound. Jaakkola and Jordan (2000) developed a seminal Variational Bayes algorithm which relies on a quadratic lower bound for the likelihood of logistic regression.
```{r,echo=FALSE,warning=FALSE,include=FALSE}
################################## CAVI ########################################
# true parameter values i.e. betas
beta = c(1,1,1,1,1)

# prior hyperparameters
prior = list(mu = rep(0,5), Sigma = diag(1,5)) # zero mean and identity covariance matrix

# Variational Inference(SVI) parameter settings
iter = 5000 # number of iterations
tau = 1 # delay 
kappa = 0.75 # forgetting rate
```

```{r,echo=FALSE,warning=FALSE,include=FALSE}
set.seed(123)

n = 20 # sample size
x1 = runif(n, -1, 1)
x2 = runif(n, -1, 1)
x3 = runif(n, -1, 1)
x4 = runif(n, -1, 1)
X = cbind(1, x1,x2,x3,x4) # the design matrix including the intercept
y = rbinom(n, 1, prob = plogis(X %*% beta))
```


```{r,echo=FALSE,warning=FALSE,include=FALSE}
logit_CAVI = function(X,y,prior,tol = 1e-16, maxiter=10000){
  if (is.null(n = nrow(X))) stop("'X' should be a matrix, not a vector")
  
  # compute the log-determinant of a matrix
  ldet = function(X){
    if(!is.matrix(X)) return(log(X))
    determinant(X, logarithm = T)$modulus
  }
  
  lowerbound = numeric(maxiter) # ELBO
  p = ncol(X) # number of betas
  
  P = solve(prior$Sigma)
  mu = prior$mu
  Pmu = c(P %*% mu)
  Pdet = ldet(P)
  
  # initialization for omega = 0.25
  P_vb = crossprod(X*rep(0.25, n), X) + P
  Sigma_vb = solve(P_vb)
  mu_vb = Sigma_vb %*% (crossprod(X, y - 0.5) + Pmu)
  eta = c(X %*% mu_vb)
  xi = sqrt(eta^2 + rowSums(X %*% Sigma_vb * X))
  omega = tanh(xi/2) / (2 * xi)
  omega[is.nan(omega)] = 0.25
  
  lowerbound[1] = 0.5*p + 0.5*ldet(Sigma_vb) + 0.5*Pdet -
    0.5*t(mu_vb - mu)%*%P%*%(mu_vb - mu) +
    sum((y-0.5)*eta +log(plogis(xi)) - 0.5*xi) -
    0.5*sum(diag(P %*% Sigma_vb))
  
  # iterative procedure
  for (t in 2:maxiter){
    P_vb = crossprod(X*omega, X) + P
    Sigma_vb = solve(P_vb)
    mu_vb = Sigma_vb %*% (crossprod(X, y - 0.5) + Pmu)
    
    # update xi
    eta = c(X %*% mu_vb)
    xi = sqrt(eta^2 + rowSums(X %*% Sigma_vb * X))
    omega = tanh(xi/2) / (2 * xi)
    omega[is.nan(omega)] = 0.25
    
    lowerbound[t] = 0.5*p + 0.5*ldet(Sigma_vb) + 0.5*Pdet 
    - 0.5*t(mu_vb - mu)%*%P%*%(mu_vb - mu) + sum((y-0.5)*eta +
                                                   log(plogis(xi)) - 0.5*xi) - 
      0.5*sum(diag(P %*% Sigma_vb))
    
    if(abs(lowerbound[t] - lowerbound[t-1]) < tol) 
      return(list(mu = matrix(mu_vb,p,1),
                  Sigma = matrix(Sigma_vb,p,p),
                  Convergence = cbind(Iteration=(1:t)-1,
                                      Lowerbound=lowerbound[1:t]), 
                  xi=xi))
  }
  stop("The algorithm has not reached convergence")
}
```

```{r,echo=FALSE,warning=FALSE,include=FALSE}
set.seed(123)

CAVI_output = logit_CAVI(X = X, y = y, prior = prior)

# Posterior distribution of the intercept with CAVI
beta0_CAVI = rnorm(iter, CAVI_output$mu[1], sqrt(CAVI_output$Sigma[1,1])) 

# Posterior distribution of the slope with CAVI
beta1_CAVI = rnorm(iter, CAVI_output$mu[2], sqrt(CAVI_output$Sigma[2,2])) 

beta2_CAVI = rnorm(iter, CAVI_output$mu[3], sqrt(CAVI_output$Sigma[3,3]))

beta3_CAVI = rnorm(iter, CAVI_output$mu[4], sqrt(CAVI_output$Sigma[4,4]))

beta4_CAVI = rnorm(iter, CAVI_output$mu[5], sqrt(CAVI_output$Sigma[5,5]))

# Posterior distribution of the intercept with SVI
# beta0_SVI  <- rnorm(10^4, SVI_output$mu[1], sqrt(SVI_output$Sigma[1,1]))   

# Posterior distribution of the slope with SVI
# beta1_SVI  <- rnorm(10^4, SVI_output$mu[2], sqrt(SVI_output$Sigma[2,2]))   

data_plot = data.frame(Posterior = c(beta0_CAVI,beta1_CAVI,beta2_CAVI,beta3_CAVI,beta4_CAVI), 
                       beta = rep(c("beta0","beta1",'beta2','beta3','beta4'),each=iter), 
                       Algorithm = rep(c("CAVI"),each=5*iter), 
                       Sample_size = 10)
```


```{r,echo=FALSE}
ggplot(data=data_plot, 
       aes(x = as.factor(Sample_size), 
           y = Posterior, fill=beta)) +
  facet_grid(~beta) + 
  geom_boxplot(alpha=0.5, show.legend = F) + 
  theme_minimal()+
  geom_hline(yintercept=1, linetype="dotted") + 
  xlab("Sample size") + 
  ylab("Logistic Regression Coefficient")

```

Here is a boxplot illstration of one-iteration CAVI algorithm with 5000 MCMC samples. This stochastic trial gives an intuitive idea of the CAVI performance for parameter estimation. We can see that except for $\beta_4$, the median values of 5000 samples are near the observed/true parameter value 1. Then, we run this single trial 200 times and see the aggregate outcome of this algorithm:

```{r,echo=FALSE,warning=FALSE,include=FALSE}
set.seed(1)
round = 200
sample_mean_VI = data.frame(beta0=double(),beta1=double(),beta2=double(),beta3=double(),beta4=double())
sample_var_VI = data.frame(beta0=double(),beta1=double(),beta2=double(),beta3=double(),beta4=double())

i=1
while(i <= round){
  CAVI_output = logit_CAVI(X = X, y = y, prior = prior)
  
  beta0 = rnorm(iter, CAVI_output$mu[1], sqrt(CAVI_output$Sigma[1,1]))
  mean_beta0 = mean(beta0)
  var_beta0 = var(beta0)
  
  beta1 = rnorm(iter, CAVI_output$mu[2], sqrt(CAVI_output$Sigma[2,2]))
  mean_beta1 = mean(beta1)
  var_beta1 = var(beta1)
  
  beta2 = rnorm(iter, CAVI_output$mu[3], sqrt(CAVI_output$Sigma[3,3]))
  mean_beta2 = mean(beta2)
  var_beta2 = var(beta2)
  
  beta3 = rnorm(iter, CAVI_output$mu[4], sqrt(CAVI_output$Sigma[4,4]))
  mean_beta3 = mean(beta3)
  var_beta3 = var(beta3)
  
  beta4 = rnorm(iter, CAVI_output$mu[5], sqrt(CAVI_output$Sigma[5,5]))
  mean_beta4 = mean(beta4)
  var_beta4 = var(beta4)
  
  sample_mean_VI = add_row(sample_mean_VI, beta0 = mean_beta0,
                          beta1 = mean_beta1,
                          beta2 = mean_beta2,
                          beta3 = mean_beta3,
                          beta4 = mean_beta4)
  sample_var_VI = add_row(sample_var_VI, beta0 = var_beta0,
                          beta1 = var_beta1,
                          beta2 = var_beta2,
                          beta3 = var_beta3,
                          beta4 = var_beta4)
  i = i+1
}

# mean-squared error of each round for each beta
MSE_VI = (sample_mean_VI - beta)^2 + sample_var_VI
round_mean_MSE_VI = sapply(MSE_VI, mean)
```

```{r,echo=FALSE,cache=TRUE}
summary(MSE_VI)
```

According to this summary table, the CAVI method draws the parameter posterior samples closely from the true posterior distribution, resulting in a very good approximation with small variances.

One sidenote observation regarding the sample size of our logistic generative model: recall that our initial setting for the sample size of binary outcomes is 10, that is, we simulate 10 logistic data as our simulation dataset to generate one MCMC sample of parameters. What if we have a larger sample size for the generative model?

```{r,echo=FALSE,warning=FALSE,include=FALSE}
nn <- c(1000, 5000, 10000) # setting the sample size
data_plots = data.frame()
for(n in nn){
  set.seed(123)      # Set the seed to make this experiment reproducible
  x1 <- runif(n,-1,1)
  x2 <- runif(n,-1,1)
  x3 <- runif(n,-1,1)
  x4 <- runif(n,-1,1)
  X <- cbind(1,x1,x2,x3,x4)    
  y <- rbinom(n,1,prob = plogis(X%*%beta))

  set.seed(1010)     # Set the seed to make this experiment reproducible
  CAVI_output <- logit_CAVI(X = X, y = y, prior = prior) # CAVI 
  
  #SVI_output  <- logit_SVI(X = X, y = y,  prior = prior,  
                           #iter = iter, tau = tau, kappa = kappa) # SVI

  set.seed(100)
  
  # Posterior distribution of the intercept with CAVI
  beta0_CAVI <- rnorm(10^4, CAVI_output$mu[1], sqrt(CAVI_output$Sigma[1,1]))
  
  # Posterior distribution of the slope with CAVI
  beta1_CAVI <- rnorm(10^4, CAVI_output$mu[2], sqrt(CAVI_output$Sigma[2,2])) 
  
  beta2_CAVI = rnorm(10^4, CAVI_output$mu[3], sqrt(CAVI_output$Sigma[3,3]))

  beta3_CAVI = rnorm(10^4, CAVI_output$mu[4], sqrt(CAVI_output$Sigma[4,4]))

  beta4_CAVI = rnorm(10^4, CAVI_output$mu[5], sqrt(CAVI_output$Sigma[5,5]))
  
  # Posterior distribution of the intercept with SVI
  #beta0_SVI  <- rnorm(10^4, SVI_output$mu[1], sqrt(SVI_output$Sigma[1,1]))  
  
  # Posterior distribution of the slope with SVI
  #beta1_SVI  <- rnorm(10^4, SVI_output$mu[2], sqrt(SVI_output$Sigma[2,2]))   

  data_plots <- rbind(data_plots,data.frame(Posterior = c(beta0_CAVI,beta1_CAVI,
                                                         beta2_CAVI, beta3_CAVI, beta4_CAVI),
                                          beta = rep(rep(c("beta0","beta1",'beta2','beta3','beta4'),
                                                         each=10^4),5), 
                                          Algorithm = rep(c("CAVI"),
                                                          each=5*10^4), 
                                          Sample_size = n))
}
```

```{r,echo=FALSE}
ggplot(data=data_plots, 
       aes(x = as.factor(Sample_size), 
           y = Posterior,color=Sample_size)) + 
  facet_grid(~beta) + 
  geom_boxplot(alpha=0.7,show.legend = F) + 
  theme_minimal() +
  geom_hline(yintercept=1, linetype="dotted") + 
  xlab("Sample size") + 
  ylab("Regression Coefficient")
```

I tried to increase the number of binary data being generated to 1000, 5000 and 10000. Running the same CAVI algorithm in one iteration with 5000 MCMC samples of parameters, the boxplot clearly illustrates a convergence to the true/observed parameter values and an descending variance as we increase the sample size of data, which follows our intuition. The following histogram confirms our observations:


```{r,echo=FALSE}
ggplot(data=data_plots, 
       aes(x = Posterior, color = beta, fill=beta)) + 
  geom_histogram(alpha=0.5,binwidth = 0.005)+
  facet_wrap(~Sample_size,ncol = 1)+
  theme_minimal()+
  geom_vline(aes(xintercept = 1))
```


## Comparisons and Results

In summary, the following plot shows the final result: the 200-iteration-average values of MSE for all of the algorithms mentioned above. More trial-and-error simulations and research can be conducted on the choice of summary statistics for categorical data and optimization methods for time efficiency, and other metrics to survey the accuracy of parameter estimation problems.

```{r,echo=FALSE,cache=TRUE}
library(tidyr)

convert = function(x){
  return(as.data.frame(t(as.data.frame(x,optional=T))))
}

PG = convert(round_mean_MSE_PG)
ABC = convert(round_mean_MSE_ABC)
VI = convert(round_mean_MSE_VI)
BSL = convert(round_mean_MSE_BSL)

PG1 = gather(PG,columnNames,values)
PG1$algo = rep('MCMC',5)
ABC1 = gather(ABC,columnNames,values)
ABC1$algo = rep('ABC',5)
VI1 = gather(VI,columnNames,values)
VI1$algo = rep('VI',5)
BSL1 = gather(BSL, columnNames, values)
BSL1$algo = rep('BSL',5)

compare = rbind(PG1,ABC1,VI1,BSL1)
compare %>% 
  ggplot(aes(x=columnNames, y = values,group = algo,color = algo))+
  geom_line()+
  geom_point()+
  geom_text(aes(label = round(values,3)), vjust=-0.5)+
  theme_minimal()+
  ylab('MSE')+xlab('Parameter')
```


## References:

Blei, D. M., Kucukelbir, A., & McAuliffe, J. D. (2017). Variational inference: A review for statisticians. Journal of the American statistical Association, 112(518), 859-877.

Choi, H. M., & Hobert, J. P. (2013). The Polya-Gamma Gibbs sampler for Bayesian logistic regression is uniformly ergodic. Electronic Journal of Statistics, 7, 2054-2064.

Durante, D., & Rigon, T. (2019). Conditionally conjugate mean-field variational Bayes for logistic models. Statistical science, 34(3), 472-485.

Jaakkola, T.S., Jordan, M.I. Bayesian parameter estimation via variational methods . Statistics and Computing 10, 25–37 (2000). https://doi.org/10.1023/A:1008932416310

Marin, J. M., Pudlo, P., Robert, C. P., & Ryder, R. J. (2012). Approximate Bayesian computational methods. Statistics and Computing, 22(6), 1167-1180.

Marjoram, P., Molitor, J., Plagnol, V., & Tavaré, S. (2003). Markov chain Monte Carlo without likelihoods. Proceedings of the National Academy of Sciences, 100(26), 15324-15328.

Polson, N. G., Scott, J. G., & Windle, J. (2013). Bayesian inference for logistic models using Pólya–Gamma latent variables. Journal of the American statistical Association, 108(504), 1339-1349.

Price, L. F., Drovandi, C. C., Lee, A., & Nott, D. J. (2018). Bayesian synthetic likelihood. Journal of Computational and Graphical Statistics, 27(1), 1-11.

## Code Appendix

```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```